<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The INNR Monologue | Christopher JJ. Wang </title> <meta name="author" content="Christopher JJ. Wang"> <meta name="description" content="A introduction to Implicit Neural Neural Representation (INNR) models"> <meta name="keywords" content="deep-learning, machine-learning, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://christopher-wang.github.io/blog/2023/innr/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The INNR Monologue",
            "description": "A introduction to Implicit Neural Neural Representation (INNR) models",
            "published": "November 01, 2023",
            "authors": [
              
              {
                "author": "Christopher Wang",
                "authorURL": "https://christopher-wang.github.io",
                "affiliations": [
                  {
                    "name": "Carleton University, Kinaxis",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Christopher</span> JJ. Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/patents/">Patents </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The INNR Monologue</h1> <p>A introduction to Implicit Neural Neural Representation (INNR) models</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#hypernetworks">HyperNetworks</a> </li> <li> <a href="#implicit-neural-representations">Implicit Neural Representations</a> </li> </ul> <div> <a href="#implicit-neural-neural-representation-innr-models">Implicit Neural Neural Representation (INNR) models</a> </div> <ul> <li> <a href="#initial-mnist-model">Initial MNIST Model</a> </li> <li> <a href="#innr-model-training">INNR Model Training</a> </li> </ul> <div> <a href="#results">Results</a> </div> <div> <a href="#attacking-the-innr-model">Attacking the INNR model</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>For practitioners of Artificial Intelligence it is often said that the North Star of our efforts is Artificial General Intelligence (AGI). However, this rather simple goal is itself incredibly ill-defined and nebulous. There exists a large body of work that discusses the topic of the meaning of AGI and what it would mean for a machine to be truly considered intelligent. Today, I offer my own humble meaning on the topic: I believe AGI is when a machine approximates the function of a human being. What do I mean by this?</p> <p>The beauty of human beings and my fascination with them (and by extension AI) is the diversity and similarity between people. Remarkably, despite a great and wondrous variety of thought and circumstance, there exists a remarkable amount of overlap and commonality between people that unify the human experience. It appears that for many people, they share in learning many of the same things, resulting in cliches that while boring remain true for most if not all peoples: it is likely that all of us at some point will learn through our own experiences what it means to feel pride, joy, hunger, and grief.</p> <p>Indeed, these patterns to me speak of a greater overarching idea: there is some process by which humans have to go through while on the path to acquire cognition à la general intelligence. If we consider an individual human’s consciousness as a datum, it is this process which I believe the field should aim to replicate for machines.</p> <h2 id="background">Background</h2> <p>So how can we accomplish this goal? I turn to the recent advancements of deep learning and neural networks. Heavily oversimplifying, the aim of a neural network is to learn some particular data generating process. By now I’m sure you’re aware of the common story of learning by example, whereby we feed a neural network many examples of a particular tasks and it learns through convex optimization. So in the example of an image classifier, we may feed many examples of a image and label pairs so that we may approximate the mapping (read model the data generating process) of image to label. In reality, neural networks can learn any such function (see <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="external nofollow noopener" target="_blank">Universal Approximation Theorem</a>).</p> <h3 id="hypernetworks">HyperNetworks</h3> <p>Learning a the mapping from image to label is all well and good, but in my mind to achieve true AGI we must answer the question ‘What is the data generating process for cognition?’ If we can define such a function then is it possible to learn it? The answer (unsurprisingly) is yes! Metalearning is a field that aims to address these questions. While there are a variety of different approaches to meta-learning, in this blog post I aim to focus on one: <em>hypernetworks</em>.</p> <p>Hypernetworks are neural networks that have been trained to generate the weights (either in full or in part) of other networks. It is distinct from meta-learning algorithms like MAML<d-cite key="finn2017modelagnostic"></d-cite> which aim to learn a set of weights that may transfer across tasks (GPTs are trained using this paradigm) or Neural Architecture Search<d-cite key="zoph2017neural"></d-cite> which aim to learn the architecture but not the weights. Instead, we are either implicitly or explicitly modelling the distribution of the (useful) model parameters with another neural network.</p> <h3 id="implicit-neural-representations">Implicit Neural Representations</h3> <p>Switching gears a bit, Implicit Neural Representations (INR) are a class of neural networks that model discrete signals as continuous by learning a representation that maps values directly from coordinate space. For instance, a colour image is a discrete signal of \((r, g, b)\) values that is defined by \((x,\ y)\) coordinates. An implicit neural representation for an image is a function \(f(x,\ y\ |\ \theta)\) with a maximum domain of \((height,\ width)\). Notably, the domain is of fixed size with discrete pixel intervals however the learned function is continuous.</p> \[f(x,\ y\ |\ \theta) → (r_{x, y},\ b_{x, y},\ g_{x, y})\] <p>INRs have been used to map a variety of different data modalities, from images to audio to point clouds to 3D scenes. Popular examples of INRs include Neural Radiance Fields (NeRFs)<d-cite key="mildenhall2020nerf"></d-cite> and Sinusoidal Representation Networks (SIRENs)<d-cite key="sitzmann2020implicit"></d-cite>. In fact they themselves are often the output of HyperNetworks<d-cite key="kosiorek2021nerfvae"></d-cite> because of their usefulness in representing other objects and their fixed size. However, what would it mean to model a neural network as a continuous signal?</p> <h2 id="implicit-neural-neural-representation-innr-models">Implicit Neural Neural Representation (INNR) models</h2> <p>This work aims to explore that question by introduce the concept of Implicit Neural Neural Representations (INNR) Models. To my knowledge this is the first work that attempts to apply INRs to completely represent a neural network. INNR models are a class of hypernetwork that only encode for a single neural network. These models can be thought of as attempting reconstruct the original model ‘signal’ in a lossy way. The idea is that it may be possible to use INNR models to construct larger models. The goals of this work are as follows.</p> <ol> <li>Identify the feasibility of representing neural network architectures as a continuous signal.</li> <li>Explore the feasibility of INNR models as a form of network compression that does not require any training data other than the weights of the model itself.</li> <li>Understand how INNR models may be vulnerable to evasion attacks from their original model.</li> </ol> <h3 id="initial-mnist-model">Initial MNIST Model</h3> <p>To begin, we must define an initial model for which we will attempt to compress. For this exploration we will consider only the perennial toy dataset MNIST. Shown below is a simplified convolutional neural network that is loosely inspired by the LeNet5 architecture.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class MnistExampleNet(nn.Module):
  """
  A simple convolutional network inspired by LeNet5 for MNIST classification

  Parameters:
      c (int): Number of channels in the first convolutional layer.
  """
  def __init__(self, c: int=32):
    super(MnistExampleNet, self).__init__()
    self.conv1 = nn.Conv2d(1, c, kernel_size=5, bias=False)
    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
    self.conv2 = nn.Conv2d(c, c * 2, kernel_size=5, bias=False)
    self.fc1 = nn.Linear(c * 2 * 4 * 4, 10, bias=False)

  def forward(self, x):
    x = self.pool(F.sigmoid(self.conv1(x)))
    x = self.pool(F.sigmoid(self.conv2(x)))
    x = x.flatten(1)
    x = self.fc1(x)
    return x
</code></pre></div></div> <p>We train on this model for only 5 epochs, which is sufficient to achieve an adequate convergence for the sake of our testing. The training and accuracy curves of our initial model can be found below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-11-01-innr/initial-model-curves-480.webp 480w,/assets/img/2023-11-01-innr/initial-model-curves-800.webp 800w,/assets/img/2023-11-01-innr/initial-model-curves-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-11-01-innr/initial-model-curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="innr-model-training">INNR Model Training</h3> <p>We must now decide on a coordinate scheme to train our INNR model. This is challenging as the dimensionality of a network changes depending on the type of layer: convolutional layers have 4 dimension , linear layers have 2 dimensions, and bias layers have a dimension of 1. The simplest mechanism to map these coordinates is to project the lower dimensional linear and bias layers as a hyperplane centered around the origin. Note that the domain varies significantly across layers, with \((channel\ out, channel\ in, filter\ height, filter\ width)\) as the domain of convolutional layers, \((output,\ input)\) as the domain of linear layers, and \((output)\) as the domain of bias layers.</p> <p>Therefore our coordinate scheme is as follows:</p> <ul> <li>\((layer,\ c_{out},\ c_{in}, w,\ h)\) for convolutional layers</li> <li>\((layer,\ out,\ in,\ 0,\ 0)\) for linear layers</li> <li>\((layer,\ out,\ 0,\ 0,\ 0)\) for bias layers (omitted for simplicity)</li> </ul> <p>Given that the domain is variable across layers its likely that the manifold learned by the INNR model is more intricate compared to models dealing with consistent domains, such as images or audio. We normalize the domain of each dimension of a layer to \([-1,\ 1]\). The code for this dataset can be found below.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class ConvCoordinateDataset(Dataset):
  """
  Dataset class for handling neural network coordinates and weights.

  Args:
    params (Tuple[torch.Tensor]): Tuple of parameter tensors
    standardize (bool): Flag indicating whether to standardize the weights.
  """
  def __init__(self, params: Tuple[torch.Tensor], standardize: bool = True):
    self.weights = self._flatten_params(params)
    self.coordinates = self._convert_to_coordinates(params)
    self.standardize = standardize
    if standardize:
      self.transformed_weights = self.standardize_weights()

  # Flatten the paramneter tuple
  def _flatten_params(self, params: Tuple[torch.Tensor]):
    flat_params = torch.concat([param.cpu().flatten() for param in params])
    return flat_params.view(-1, 1)

  # Translate convolutional params into normalized coordinates (cout, cin, h, w)
  def _conv_coordinates(self, param: torch.Tensor) -&gt; torch.Tensor:
    conv_ranges = [torch.linspace(-1, 1, steps=size) for size in param.shape]
    mgrids = torch.meshgrid(*conv_ranges)
    normalized_coordinates = torch.stack(mgrids, dim=-1).reshape(-1, 4)
    return normalized_coordinates

  # Translate bias params into normalized coordinates (bias, 0, 0, 0)
  def _bias_coordinates(self, param: torch.Tensor) -&gt; torch.Tensor:
    zeroes = torch.zeros(1)
    bias_ranges = torch.linspace(-1, 1, steps=param.shape[0])
    mgrids = torch.meshgrid(bias_ranges, zeroes, zeroes, zeroes)
    normalized_coordinates = torch.stack(mgrids, dim=-1).reshape(-1, 4)
    return normalized_coordinates

  # Translate linear parameters into normalized coordinates (out, in, 0, 0)
  def _lin_coordinates(self, param: torch.Tensor) -&gt; torch.Tensor:
    zeroes = torch.zeros(1)
    lin_ranges = [torch.linspace(-1, 1, steps=size) for size in param.shape]
    mgrids = torch.meshgrid(*lin_ranges, zeroes, zeroes)
    normalized_coordinates = torch.stack(mgrids, dim=-1).reshape(-1, 4)
    return normalized_coordinates

  # Generate Coordinates with range [-1, 1] in form (layer, out, in, h, w)
  def _convert_to_coordinates(self, params: Tuple[torch.Tensor]):
    layer_coords = torch.linspace(-1, 1, steps=len(params))
    coordinates = []
    for param in params:
      if param.ndim == 4:
        coordinates.append(self._conv_coordinates(param))
      if param.ndim == 1:
        coordinates.append(self._bias_coordinates(param))
      if param.ndim == 2:
        coordinates.append(self._lin_coordinates(param))
    for i in range(len(params)):
      layer_coord = torch.full((len(coordinates[i]), 1), layer_coords[i])
      coordinates[i] = torch.concat([coordinates[i], layer_coord], -1)
    return torch.concat(coordinates)

  # Standardize outputs to improve training convergence
  def standardize_weights(self):
    mean, std = self.weights.mean(), self.weights.std()
    standardized_weights = (self.weights - mean) / std
    return standardized_weights

  # Reverse standardization for inference time
  def inverse_standardize_weights(self, weights: torch.Tensor):
    mean, std = self.weights.mean(), self.weights.std()
    standardized_weights = weights * std + mean
    return standardized_weights

  def __len__(self):
    return len(self.coordinates)

  def __getitem__(self, idx):
    weights = self.transformed_weights if self.standardize else self.weights
    return self.coordinates[idx], weights[idx]
</code></pre></div></div> <p>The learned INNR model is of the form \(f(l,\ out,\ in,\ x,\ y\ |\ \theta) → w_{(l,\ out,\ in,\ x,\ y)}\) where $w$ is the parameter at the given layer coordinates. I use the popular SIREN<d-cite key="sitzmann2020implicit"></d-cite> architecture for the INNR model due to its strong performance on a variety of representation tasks. Unlike the initial model where convergence can be considered arbitrary, for the INNR model I train with a full <em>2000</em> epochs. However, notably this training does not require any data used during the training of the initial model, only the raw weights. This means this method could potentially be applied in a variety of settings where data is a protected citizen. Shown below are the loss curves of the INNR Model Training.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-11-01-innr/innr-model-curves-480.webp 480w,/assets/img/2023-11-01-innr/innr-model-curves-800.webp 800w,/assets/img/2023-11-01-innr/innr-model-curves-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-11-01-innr/innr-model-curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="results">Results</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-11-01-innr/parameter-comparison-480.webp 480w,/assets/img/2023-11-01-innr/parameter-comparison-800.webp 800w,/assets/img/2023-11-01-innr/parameter-comparison-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-11-01-innr/parameter-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>As shown above we see that the performance of the INNR model’s produced model is comparably to the initial model while having roughly 20\% fewer parameters. The difference in performance was expected as INRs are lossy when encoding their mediums. While other techniques may result in smaller networks of comparable quality, what makes this method notable was the exclusion of the any <em>training or testing</em> data. The INNR model fully encoded the initial convolutional neural network. However, since we are directly approximating the target network, it does mean that we also reconstruct the potential vulnerabilities of the initial models as well.</p> <h2 id="attacking-the-innr-model">Attacking the INNR model</h2> <p>An adversarial example is a manipulated input data sample that has been perturbed to cause an unexpected outcome. This is typically done by backpropagation through a target network. As our INNR model attempts to approximate the initial network, it too should be vulnerable to any attacks that are generated from the initial network. We test out this hypothesis empirically. For this task we use the Fast Gradient Sign Method introduced by GoodFellow et al. to generate the adversarial attacks. The Fast Gradient Sign Method is denoted by \(adv_x = x + ϵ*sign(\nabla_xL(θ, x, y))\) where \(x\) is the input image, \(y\) is either a directed or undirected adversarial target, \(θ\) are the parameters of the attacked network, \(adv_x\) is the adversarial instance of \(x\), \(L\) is a loss function, and \(ϵ\) is the maximum bounded error. Since the attack only considers a single step within a $\epsilon$ radius ball of the input image. The idea is that the perturbation can be constrained by $\epsilon$ so as to still appear visually coherent. The code used for the attack is shown below (we only consider undirected attacks).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Generates an adversarial attack using the fast gradient sign method
def attack(model: nn.Module,
          device: torch.device,
          test_loader: DataLoader,
          criterion: nn.Module,
          epsilon=0.1):
  model = model.to(device)
  model.eval()
  attacks, targets = [], []
  for batch_idx, (data, target) in enumerate(test_loader):
    data, target = data.to(device), target.to(device)
    data.requires_grad = True
    output = model(data)
    loss = criterion(output, target)
    model.zero_grad()
    loss.backward()
    attack = data + epsilon * torch.sign(data.grad.data)
    attacks.append(attack)
    targets.append(target)
  return torch.concat(attacks), torch.concat(targets)
</code></pre></div></div> <p>We investigate adversarial attacks to understand the reconstruction error of the INNR model. If the model is faithfully reconstructed then we expect the adversarial attacks to transfer, which we can see in the below figure.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-11-01-innr/attack-comparison-480.webp 480w,/assets/img/2023-11-01-innr/attack-comparison-800.webp 800w,/assets/img/2023-11-01-innr/attack-comparison-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-11-01-innr/attack-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>This project was chosen as the code snippet for review for Christopher Wang’s MILA application for Fall of 2024. This project is ideal because of its limited scope, ease of readability, and interesting results. All experiments were conducted on Google Colab using a T4. A link to the the original Google Colab is provided <a href="https://colab.research.google.com/drive/1EI7whJGWOPD1IB6qDr0yK4szOMwaIZ8B?usp=sharing" rel="external nofollow noopener" target="_blank">here</a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-11-01-innr.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Christopher-Wang/christopher-wang.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Christopher JJ. Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>