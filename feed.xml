<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://christopher-wang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://christopher-wang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-22T23:43:33+00:00</updated><id>https://christopher-wang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A personal website for Christopher Jiajun Wang. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Dual Axis</title><link href="https://christopher-wang.github.io/blog/2024/dualaxis/" rel="alternate" type="text/html" title="The Dual Axis"/><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://christopher-wang.github.io/blog/2024/dualaxis</id><content type="html" xml:base="https://christopher-wang.github.io/blog/2024/dualaxis/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Plato, in search of a way to define what comprises a human being, arrived at the conclusion that a person is ‘a featherless biped’. Diogenes, a contemporary (and a philosopher worth studying in his own right), brought a plucked chicken to Plato’s Academy and proclaimed ‘Here is Plato’s man’. In response, Plato added ‘with flat, broad nails’ to correct for the mistake. I love this story not just for Diogenes’s facetious response but also because of how it illustrates a fundamental epistemological problem: what comprises an object? If we choose to describe an object by a series of characteristics, how many do we need before we can be confident with what object we are describing? In the above example, the characteristics of ‘bipedal’ and ‘featherless’ were insufficient, and ‘flat and broad nails’ were added for further clarity.</p> <p>If we were to describe humans as belonging to a matrix of different animal characteristics, according to Plato, we would need at least three to differentiate a human from other animals. It could be said that the intrinsic dimension of a person is therefore 3. Intrinsic dimensionality is the number of variables needed to minimally represent a dataset. It is deeply related to the <a href="https://en.wikipedia.org/wiki/Manifold_hypothesis#:~:text=The%20manifold%20hypothesis%20posits%20that,inside%20that%20high%2Ddimensional%20space.">manifold hypothesis</a>, which states that for many high-dimensional datasets such as audio, vision, and language, their data-generating process actually lies on a low-dimensional manifold. This poses an interesting question: from what manifold are neural networks generated?</p> <p>Orthogonal to the question of dimensionality is the question of sparsity. It is a common myth that humans only use 10% of their brain, an absurd claim given that the brain roughly consumes 20% of your body’s overall calories. In reality, we are often using the whole of our brain; however, there does appear to be evidence to suggest that not all neurons are fired. Rather, there appears to be sparseness in which neurons are fired for any given stimulus.</p> <p>It turns out this is true for neural networks as well. While modern compute infrastructure is optimized for dense computation, it’s likely that over 90% of such computations are unnecessary. This is a major problem, especially as the demand for gigantic foundation models captures the public’s zeitgeist. Some estimates put yearly power consumption at <a href="https://www.scientificamerican.com/article/the-ai-boom-could-use-a-shocking-amount-of-electricity/"><strong>80+ terawatt-hours globally</strong></a>! While it currently accounts for roughly 1% of global power consumption (far from the human 20%), it is still a cause for concern given the shifting climate. Therefore it is a noble goal to pursue more energy efficient (read minimal) computations to achieve strong results.</p> <p>Therefore, as we aim to achieve higher levels of artificial intelligence, we must ask ourselves what is the nature of intelligence. More specifically, we pose the question of “What minimally constitutes intelligence?” This can be posed both in the context of the number of dimensions we require to represent intelligence and how many operations are necessary to perform intelligent tasks.</p> <h2 id="background">Background</h2> <p>I thought I’d tackle the background of this blog a little differently than before. As some of you may know, I have been accepted as a graduate student at the University of Alberta. My goals are to perform research on the nature of neural network training, specifically focusing on large language models. As you might expect, I will have to read a great many number of papers! In preparation for that, I will be presenting the background for this blog as paper reviews. This works out as the bulk of the work for this blog involves paper reproduction. The works that I aim to reproduce are mainly related to the nature of neural networks.</p> <h3 id="measuring-the-intrinsic-dimension-of-objective-landscapes">Measuring the Intrinsic Dimension of Objective Landscapes</h3> <p>The paper that spawned the paper with a thousand citations investigates the impact of low-rank training for a variety of different tasks and neural architectures. Given a parameter set of size \(n\), the authors employ a random projection \(\theta^{n} = \theta^{n}_{0} + P\theta^{m}\) where \(\theta^{n}\) represents the parameters of a network, \(\theta^{n}_{0}\) represents the initial parameters of a network, \(P^{n \times m}\) is a random projection matrix, and \(\theta^{m}\) represents a lower-dimensional set of trainable parameters. Cleverly, the authors initialize \(\theta^{m}\) to 0, meaning they take advantage of the original model initialization (a trick employed by many of the modern parameter-efficient fine-tuning methods we use today). They show that it is possible to achieve 90% of a fully parameterized model’s task accuracy with an incredibly compressed parameter space. Furthermore, their experiments suggest that architecture (fully connected vs convolutional networks) and task difficulty (MNIST vs Shuffled MNIST) have significant impacts on compressibility. To me, it provides a great argument for the inductive bias of a neural network architecture. Furthermore, if the data-generating process is embedded in a low-dimensional manifold, then it is only natural that neural networks themselves would fall into the manifold hypothesis as well. <d-cite key="li2018measuring"></d-cite></p> <h3 id="intrinsic-dimensionality-explains-the-effectiveness-of-language-model-fine-tuning">Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</h3> <p>The paper that actually spawned the paper with a thousand citations explores the intrinsic dimensionality of fine-tuning from a set of pre-trained weights. Specifically, they tune a RoBERTa model following a similar methodology to Li et al. Distinctly, they augment the random projections of Li et al.’s previous work to \(\theta^{n}_{i} = \theta^{n}_{0, i} + \lambda_{i}P\theta^{d-m}_{i}\) where they add a trainable parameter \(\lambda_i\) for the \(i\)-th layer, and \(m\) is the number of layers. This is to allow the algorithm (called Structurally Aware Intrinsic Dimension or SAID) to dampen layers that are unneeded in the fine-tuning. To enable the size of \(P\), they perform a Fast Food transform instead of a dense random projection. Once again, they found that the intrinsic dimension was tied to the difficulty of the fine-tuning task. This served as a direct inspiration for the intuition behind the development of Low-Rank Adaptation (LORA) <d-cite key="hu2021lora"></d-cite> and serves as a grandfather paper to the field of parameter-efficient fine-tuning. Key to the idea is that the pre-training stage serves as a strong conditioning where fine-tuning only requires minimal traversal from the conditioned state. Overall, not a surprising paper but an important one all the same. <d-cite key="aghajanyan2020intrinsic"></d-cite></p> <h3 id="the-lottery-ticket-hypothesis-finding-sparse-trainable-neural-networks">The Lottery Ticket Hypothesis: Finding Sparse, Trainable neural Networks</h3> <p>In this work, the authors propose the Lottery Ticket hypothesis. This hypothesis states that there exists a subnetwork within an over-parameterized dense model that, when initialized and trained, can match the accuracy of the original model. They then go further and propose a conjecture that states that SGD seeks to find these subnetworks and trains them. In support of their argument, they found that they may iteratively prune a network by first training a model, pruning the model, then resetting the subnetwork to its initial weights, which may then be trained to equal or greater accuracy in equal or fewer training iterations. They call this methodology of training, pruning, and resetting Iterative Magnitude Pruning (IMP). Interestingly, they show that while a pruned network may be reinitialized and trained from scratch, this fails at a certain level of sparsity, and the weight initialization does have an impact on the performance of the retraining. While they experiment on both a feed-forward and convolutional neural network, they do so on different datasets. I would be curious to see how the model architecture affects the sparsity rates that may be achieved. They did, however, experiment with pruning only the feed-forward or convolution weights. However, as the feed-forward weights vastly outnumber the convolutional weights, I feel as though this is not an apt comparison (they were able to achieve better performance on pruning the convolutional layers, see appendix H.6). <d-cite key="frankle2019lottery"></d-cite></p> <h3 id="deconstructing-lottery-tickets-zeros-signs-and-the-supermask">Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask</h3> <p>In this work, the authors explore the dynamics of the lottery ticket through several different experiments. First, they begin by defining several different Mask Criteria to derive a suitable mask for a neural network. They then found the following:</p> <ol> <li>Initialization is not critical to the performance of iterative pruning. Although still lower in accuracy than keeping the initial weights, keeping the sign is sufficient for providing good results.</li> <li>Masking out weights (Hadamard product with 0) greatly improves performance over simply freezing the initial weight. This is interesting because it supports the idea that the weight being masked was already being shifted towards zero in the initial stages of optimization.</li> <li>They show the existence of supermasks, masks that when applied to an untrained set of weights provide greatly improved performance over random chance. Notably, this supermask is still derived from the training process.</li> </ol> <p>While the work is interesting, I found it rather unsurprising. Beginning with the first point, it appears natural that the initial sign is important as it most defines the differences in angle. The second point appears to be clear as well; masking can be thought of as taking a gradient step equal to the original weight size. Certainly, a few weights were set to be zero. Finally, given that the supermask was derived from the training process, at a minimum, it would be equivalent to performing gradient descent for the 0 terms. <d-cite key="zhou2020deconstructing"></d-cite></p> <h2 id="the-dual-axis">The Dual Axis</h2> <p>So, why do we care? Well, you can think of dimensionality and sparsity as representing the intrinsic number of parameters necessary to complete a task. There has been quite a large body of work that explores either dimensionality or sparsity. Interestingly, both have deep motivations stemming from the drive to discover more efficient and faster algorithms. As I alluded to earlier, the investigations into dimensionality have exploded with the advent of large language models. Their ability to generalize to new, unseen tasks with little (in the context of fine-tuning) to no (in the case of zero-shot prompting) additional training is extraordinary in the field of deep learning. Therefore, a rich ecosystem has arisen in discovering new parameter-efficient fine-tuning methods that leverage the lower dimensionality of solving tasks.</p> <p>In contrast, network pruning has a more storied history, with investigations dating back to 1989 with Le Cun’s investigation into optimal brain damage.<d-cite key="brain"></d-cite> Even back then, it was understood that not all of a model’s weights were necessary and some could be removed. Network pruning therefore arose as a method for reducing the size and complexity of a neural network. However, it has mainly been hindered by the fact that modern architecture prefers to perform dense matrix computations.</p> <p>Combined, they define two very different but related ideas: what are the minimal number of parameters necessary to perform a task? Here, I would like to introduce the <strong>dual axis of dimension and sparsity</strong>. Dimension addresses the question of ‘What is the minimum set of attributes do I need to describe a concept?’ Sparsity, however, addresses the problem of ‘How many numbers do I need in order to determine the attributes?’ Let’s try to uncover some of the interplay between these two axes.</p> <h3 id="initial-mnist-model">Initial MNIST Model</h3> <p>To begin, we must define an initial model for which we will attempt to compress. Below is a simplified convolutional neural network that is loosely inspired by the LeNet5 architecture. For those of you who’ve read my previous work, you will recognize this model. What I would distinctly like to draw attention to is the difference in activation function for this model: I am now using ReLU. The reason for this is because for the intrinsic dimensionality, we must propagate backwards through a massive projection matrix \(P\). Therefore, I chose to use ReLU to improve gradient flow and numerical stability.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class MnistExampleNet(nn.Module):
  """
  A simple convolutional network inspired by LeNet5 for MNIST classification

  Parameters:
      c (int): Number of channels in the first convolutional layer.
  """
  def __init__(self, c: int=32):
    super(MnistExampleNet, self).__init__()
    self.conv1 = nn.Conv2d(1, c, kernel_size=5, bias=False)
    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
    self.conv2 = nn.Conv2d(c, c * 2, kernel_size=5, bias=False)
    self.fc1 = nn.Linear(c * 2 * 4 * 4, 10, bias=False)

  def forward(self, x):
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = x.flatten(1)
    x = self.fc1(x)
    return x
</code></pre></div></div> <h3 id="methodology">Methodology</h3> <p>I now introduce our experimental methodology. First, I initialize and store an initial model. This model will serve as the initial parameter set shared across all of our experiments. I set the baseline model as this initial model and train it for 10 epochs normally to classify MNIST images. Following this, I evaluate three distinct training paradigms using the initial model’s parameters prior to training:</p> <ol> <li>Training using Intrinsic Dimensionality (ID)</li> <li>Training using Iterative Magnitude Pruning (IMP)</li> <li>Training using both Intrinsic Dimensionality and Iterative Magnitude Pruning (ID + IMP)</li> </ol> <p>For each methodology, we train for 10 epochs, with IMP and ID IMP having 4 pruning rounds. To my knowledge, this is the first work that aims to combine both intrinsic dimensionality training with iterative magnitude pruning. Below is the code for the third methodology. To implement this behavior, we use the newly ported <code class="language-plaintext highlighter-rouge">torch.func.functional_call</code>. This enables us to easily decouple the functional form of a module from its weights, which are managed by the state dictionary and saved as an attribute. Understanding this code will give you a good understanding of how I implemented the first two methodologies.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class FunctionalPrunedInstrinsicWrapper(nn.Module):
    def __init__(self, module, intrinsic_dimension, prune_ratio):
        """
        Wrapper to estimate the intrinsic dimensionality of the
        objective landscape for a specific task given a specific model
        :param module: pytorch nn.Module
        :param intrinsic_dimension: dimensionality within which we search for solution
        """
        super(FunctionalPrunedInstrinsicWrapper, self).__init__()
        theta = nn.Parameter(torch.zeros((intrinsic_dimension, )))
        self.module = module
        self.prune_ratio = prune_ratio
        self.register_parameter("theta", theta)

        for name, param in module.named_parameters():
            name = name.replace('.', '-')
            if param.requires_grad:
              param.requires_grad=False
              initial_weight = param.clone().detach().requires_grad_(False)
              matrix_size = initial_weight.size() + (intrinsic_dimension, )
              projection = (
                    torch.randn(matrix_size, requires_grad=False)
                    / intrinsic_dimension ** 0.5
                )
              self.register_buffer(f'initial_{name}', initial_weight)
              self.register_buffer(f'mask_{name}', torch.ones(initial_weight.size()))
              self.register_buffer(f'projection_{name}', projection)

    def forward(self, x):
        state_dict = {}
        for name, _ in self.module.named_parameters():
            n = name.replace('.', '-')
            V = torch.matmul(getattr(self, f'projection_{n}'), self.theta)
            param = getattr(self, f'initial_{n}') + torch.squeeze(V, -1)
            state_dict[name] = param * getattr(self, f'mask_{n}')
        x = functional_call(self.module, state_dict, x)
        return x

    def prune_and_reinitialize(self, n_round):
        nn.init.zeros_(self.theta)
        for name, param in self.module.named_parameters():
            n = name.replace('.', '-')
            V = torch.matmul(getattr(self, f'projection_{n}'), self.theta)
            X = getattr(self, f'initial_{n}') + torch.squeeze(V, -1)
            num_prune_params = int((1-self.prune_ratio**n_round) * X.nelement())
            mask = getattr(self, f'mask_{n}')
            topk = torch.topk(torch.abs(X).view(-1), k=num_prune_params, largest=False)
            mask.view(-1)[topk.indices] = 0
            setattr(self, f'mask_{n}', mask)
</code></pre></div></div> <h2 id="results">Results</h2> <h3 id="overall-comparison">Overall Comparison</h3> <p>For evaluations of methodologies 1 and 2, we aim to use the same 90% metric as was introduced in the original intrinsic dimensionality paper. Given that our initial model, once trained, hovers around 98.5% accuracy, we aim for our first two methodologies to achieve around 88.65% accuracy. For the third methodology we aim to find a lottery ticket network whereby the model performs roughly equivalent to the intial model (in this case the ID model) with the maximum amount of pruning prior to performance degradation. Below, we compare the training loss and the test accuracy across our different training methodologies. Recall that all methodologies share a set of initial weights despite traversing the optimization differently (due to the stochastic ordering of the minibatches). We note that all three training methodologies hover around the same performance and are far worse than the original baseline model. Notably, despite only having access to 256 parameters, Intrinsic Dimensionality performs favorably overall.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-29-dualaxis/all-curves-480.webp 480w,/assets/img/2024-04-29-dualaxis/all-curves-800.webp 800w,/assets/img/2024-04-29-dualaxis/all-curves-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-29-dualaxis/all-curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="comparison-of-different-pruning-rounds-for-imp">Comparison of Different Pruning Rounds for IMP</h3> <p>Following our initial assessment, we explore the varying loss curves for Iterative Magnitude Pruning. Shown below are the loss curves for the iterative magnitude pruning training methodology, with greater sparsity denoting a later pruning round. We note that, much like the findings of Zhou et al.<d-cite key="zhou2020deconstructing"></d-cite>, we discover that initial pruning actually exceeds the accuracy of the baseline across all training epochs. However, despite there being a proportionally smaller decrease in nonzero weights, we also observe a precipitous decline in performance when reaching higher levels of sparsity. This suggests that there is a minimum number of activations that must occur to adequately approximate the data-generating function. However, the sparsity is rather astounding, with only 344 trainable parameters by the final round.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-29-dualaxis/lth-curve-480.webp 480w,/assets/img/2024-04-29-dualaxis/lth-curve-800.webp 800w,/assets/img/2024-04-29-dualaxis/lth-curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-29-dualaxis/lth-curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="comparison-of-different-pruning-rounds-for-imp--id">Comparison of Different Pruning Rounds for IMP + ID</h3> <p>Following the above, we show the different pruning stages of the Intrinsic Dimensionality + Iterative Magnitude Pruning training paradigm. Unsurprisingly, we observe a similar effect to IMP whereby early pruning and reinitialization actually improves performance. However, somewhat shockingly, we see that zeroing out over half of the weights does not have a dramatic effect on model quality. Unlike in the IMP example, the total number of trainable parameters is 256 (equivalent to what was selected for the Intrinsic Dimensionality paradigm). This likely means that there is some amount of essential sparseness that must occur for useful internal model representation. This is surprising! Much like how our human brains rely on sparsity of neuronal activation, it appears that over half of all weights (and therefore computations) for a particular model are unnecessary. This is despite the entirety of the weights being a result of a random projection of a lower dimensional parameter space.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-29-dualaxis/idlth-curve-480.webp 480w,/assets/img/2024-04-29-dualaxis/idlth-curve-800.webp 800w,/assets/img/2024-04-29-dualaxis/idlth-curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-29-dualaxis/idlth-curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="adversarial-robustness-of-the-dual-axis">Adversarial Robustness of the Dual Axis</h2> <p>For those of you who’ve read my previous blog on <a href="https://christopher-wang.github.io/blog/2023/innr/">Implicit Neural Neural Representations (INNR)</a> models, you might recall that we attacked our models. Adversarial attacks can provide insights on both the nature of a neural network’s decision boundary and their underlying architecture. For instance, the work of Madry et al explores the problem of model capacity in relation to adversarial robustness.<d-cite key="madry2019deep"></d-cite> In this paper, they define the robustness as a saddle point optimization problem whereby an adversary attempts to maximize a given loss (or minimize a directed loss) while a defendant finds a model that minimizes this loss maximization. Specifically, they denote the optimization function as:</p> <p>\(\min_{\theta} \rho(\theta)\) where \(\rho(\theta) = E_{(x, y)}[\max_{\delta \in S} L(\theta, x + \delta, y)]\)</p> <p>Here, \(\rho\) an optimization function generate a set of parameters \(\theta\) that satisfies, \(S\) denotes a set of allowed perturbations for an adversary, and \(L\) is a loss function. They note that the Fast Gradient Sign Method can be considered a constrained first-order step that aims to maximize the inner function.</p> <p>Madry et al. claim that the capacity for robustness is linked to the capacity of the model. They found that small models, which may normally be able to learn a task, may not be able to learn with adversarial training. Larger models are often more robust, and the greater the capacity, the more reduced an attack may transfer. This intuitively makes sense: if we consider robustness (in this case, robustness around some \(l_{\infty}\) ball surrounding an input) as a different optimization task, you would naturally assume that you would require a larger model to encode this functionality. Here is where we finally may pose the question: ‘How does pruning and dimensionality affect robustness?’</p> <p>For this exploration, since we already introduce Madry et al let us apply Projected Gradient Descent as our attack vector. The code for projected gradient descent is shared below. Frankly it is simply a constrained iterative optimization algorithm that clamps the input to a specified size (allowable by \(S\)).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Generates an adversarial attack using the projected gradient descent method
def attack(model,
           device,
           test_loader,
           criterion,
           epsilon,
           alpha,
           attack_iter,
           clamp_min,
           clamp_max):
    model.eval()
    adversarial_examples = []
    target_labels = []

    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        data.requires_grad = True

        perturbation = torch.zeros_like(data).uniform_(-epsilon, epsilon)

        for _ in range(attack_iter):
            output = model(data + perturbation)
            loss = criterion(output, target)
            loss.backward()

            with torch.no_grad():
                perturbation += alpha * torch.sign(data.grad)
                perturbation = torch.clamp(perturbation, -epsilon, epsilon)
                perturbed_data = torch.clamp(data + perturbation, clamp_min, clamp_max)

        adversarial_examples.append(perturbed_data.detach())
        target_labels.append(target.detach())

    return torch.concat(adversarial_examples), torch.concat(target_labels)
</code></pre></div></div> <p>Below, we see some interesting patterns. It appears that pruned networks are uniquely vulnerable to adversarial attacks, even more so than the baseline model from which the attacks were generated. I believe this property to be more related to the nature of pruning than the exact gradient path a model takes (remember for ID + IMP the parameters are randomly projected back atop the initial weights). Relating back to the capacity problem: since pruned networks are in effect small subnetworks (or lottery tickets), their propensity for solving a specific individual task remains high, but their robustness to new tasks remains low. Surprisingly, intrinsic dimensionality remained quite robust to attacks. I hypothesize that this is because the gradient path that was traversed by the parameters was smoother and sufficiently different from the baseline model to offer some protection. As for the gap between the pruned ID variant and the non-pruned variant, I’m not too sure. My best guess is that the pruning of ID pushed the model to follow a similar path to IMP with some number of ‘necessary’ subnetworks being discovered then which were attacked by PGD.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-29-dualaxis/attack-comparison-pgd-480.webp 480w,/assets/img/2024-04-29-dualaxis/attack-comparison-pgd-800.webp 800w,/assets/img/2024-04-29-dualaxis/attack-comparison-pgd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-29-dualaxis/attack-comparison-pgd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>Today, we investigated the interplay between two related but separate concepts: sparsity and dimensionality. We found that we may compress dimensionality further than sparsity to achieve similar performance. We also discover that sparsity does improve performance even if the weights being pruned are the result of a random projection from a shared lower dimensional parameter space. To me, this speaks of a fundamental necessity for network size even if the solution space is found on a lower-dimensional manifold than the total network. This aligns with our intuition behind the <a href="https://en.wikipedia.org/wiki/Manifold_hypothesis#:~:text=The%20manifold%20hypothesis%20posits%20that,inside%20that%20high%2Ddimensional%20space.">manifold hypothesis</a>, suggesting that networks also fall onto a manifold much the same way the data they model do (somewhat unsurprising). Furthermore, we found that given the same initialization but different training schemes, intrinsic dimensionality is surprisingly robust to Projected Gradient Descent attacks, whereas pruned networks are uniquely vulnerable. This poses interesting questions about the nature of how these attacks are generated. A link to the original Google Colab is provided <a href="https://colab.research.google.com/drive/1tx91lJRjgmMvbqvx9u8Zu9HHMXH2NTX2?usp=sharing">here</a>.</p>]]></content><author><name>Christopher Wang</name></author><category term="peft"/><category term="adversarial"/><category term="pruning"/><summary type="html"><![CDATA[A dive into the interplay between sparsity and dimensionality]]></summary></entry><entry><title type="html">The INNR Monologue</title><link href="https://christopher-wang.github.io/blog/2023/innr/" rel="alternate" type="text/html" title="The INNR Monologue"/><published>2023-11-01T00:00:00+00:00</published><updated>2023-11-01T00:00:00+00:00</updated><id>https://christopher-wang.github.io/blog/2023/innr</id><content type="html" xml:base="https://christopher-wang.github.io/blog/2023/innr/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>For practitioners of Artificial Intelligence, it is often said that the North Star of our efforts is Artificial General Intelligence (AGI). However, this rather simple goal is itself incredibly ill-defined and nebulous. There exists a large body of work that discusses the topic of the meaning of AGI and what it would mean for a machine to be truly considered intelligent. Today, I offer my own humble perspective on the topic: I believe AGI is achieved when a machine approximates the function of a human being. What do I mean by this?</p> <p>The beauty of human beings and my fascination with them (and by extension AI) is the diversity and similarity among people. Despite a great and wondrous variety of thought and circumstance, there exists a remarkable amount of overlap and commonality between people that unify the human experience. It appears that for many people, they share in learning many of the same things, resulting in clichés that, while perhaps mundane, remain true for most, if not all, individuals. It is likely that all of us, at some point, will learn through our own experiences what it means to feel pride, joy, hunger, and grief.</p> <p>Indeed, these patterns, to me, speak of a greater overarching idea: there is some process through which humans must go while on the path to acquiring cognition akin to general intelligence. If we consider an individual human’s consciousness as a datum, it is this process that I believe the field should aim to replicate for machines.</p> <h2 id="background">Background</h2> <p>So how can we accomplish this goal? I turn to the recent advancements in deep learning and neural networks. Heavily oversimplifying, the aim of a neural network is to learn a particular data-generating process. By now, I’m sure you’re aware of the common narrative of learning by example, whereby we feed a neural network many examples of a particular task and it learns through convex optimization. So, in the example of an image classifier, we may feed many examples of image-label pairs so that we can approximate the mapping (or model the data-generating process) from image to label. In reality, neural networks can learn any such function (see <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a>).</p> <h3 id="hypernetworks">HyperNetworks</h3> <p>Learning a mapping from image to label is all well and good, but in my mind, to achieve true AGI, we must answer the question: “What is the data-generating process for cognition?” If we can define such a function, then is it possible to learn it? The answer (unsurprisingly) is yes! Metalearning is a field that aims to address these questions. While there are a variety of different approaches to meta-learning, in this blog post, I aim to focus on one: <em>hypernetworks</em>.</p> <p>Hypernetworks are neural networks that have been trained to generate the weights (either in full or in part) of other networks. It is distinct from meta-learning algorithms like MAML<d-cite key="finn2017modelagnostic"></d-cite> which aim to learn a set of weights that may transfer across tasks (GPTs are trained using this paradigm) or Neural Architecture Search<d-cite key="zoph2017neural"></d-cite> which aim to learn the architecture but not the weights. Instead, we are either implicitly or explicitly modeling the distribution of the (useful) model parameters with another neural network.</p> <h3 id="implicit-neural-representations">Implicit Neural Representations</h3> <p>Switching gears a bit, Implicit Neural Representations (INR) are a class of neural networks that model discrete signals as continuous by learning a representation that maps values directly from coordinate space. For instance, a color image is a discrete signal of \((r, g, b)\) values that is defined by \((x,\ y)\) coordinates. An implicit neural representation for an image is a function \(f(x,\ y\ |\ \theta)\) with a maximum domain of \((height,\ width)\). Notably, the domain is of fixed size with discrete pixel intervals; however, the learned function is continuous.</p> \[f(x,\ y\ |\ \theta) → (r_{x, y},\ b_{x, y},\ g_{x, y})\] <p>INRs have been used to map a variety of different data modalities, from images to audio to point clouds to 3D scenes. Popular examples of INRs include Neural Radiance Fields (NeRFs)<d-cite key="mildenhall2020nerf"></d-cite> and Sinusoidal Representation Networks (SIRENs)<d-cite key="sitzmann2020implicit"></d-cite>. In fact, they themselves are often the output of HyperNetworks<d-cite key="kosiorek2021nerfvae"></d-cite> because of their usefulness in representing other objects and their fixed size. However, what would it mean to model a neural network as a continuous signal?</p> <h2 id="implicit-neural-neural-representation-innr-models">Implicit Neural Neural Representation (INNR) models</h2> <p>This work aims to explore that question by introducing the concept of Implicit Neural Neural Representations (INNR) Models. To my knowledge, this is the first work that attempts to apply INRs to completely represent a neural network. INNR models are a class of hypernetwork that only encodes for a single neural network. These models can be thought of as attempting to reconstruct the original model ‘signal’ in a lossy way. The idea is that it may be possible to use INNR models to construct larger models. The goals of this work are as follows:</p> <ol> <li>Identify the feasibility of representing neural network architectures as a continuous signal.</li> <li>Explore the feasibility of INNR models as a form of network compression that does not require any training data other than the weights of the model itself.</li> <li>Understand how INNR models may be vulnerable to evasion attacks from their original model.</li> </ol> <h3 id="initial-mnist-model">Initial MNIST Model</h3> <p>To begin, we must define an initial model for which we will attempt to compress. For this exploration, we will consider only the perennial toy dataset MNIST. Shown below is a simplified convolutional neural network that is loosely inspired by the LeNet5 architecture.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class MnistExampleNet(nn.Module):
  """
  A simple convolutional network inspired by LeNet5 for MNIST classification

  Parameters:
      c (int): Number of channels in the first convolutional layer.
  """
  def __init__(self, c: int=32):
    super(MnistExampleNet, self).__init__()
    self.conv1 = nn.Conv2d(1, c, kernel_size=5, bias=False)
    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
    self.conv2 = nn.Conv2d(c, c * 2, kernel_size=5, bias=False)
    self.fc1 = nn.Linear(c * 2 * 4 * 4, 10, bias=False)

  def forward(self, x):
    x = self.pool(F.sigmoid(self.conv1(x)))
    x = self.pool(F.sigmoid(self.conv2(x)))
    x = x.flatten(1)
    x = self.fc1(x)
    return x
</code></pre></div></div> <p>We train this model for only 5 epochs, which is sufficient to achieve adequate convergence for the sake of our testing. The training and accuracy curves of our initial model can be found below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-11-01-innr/initial-model-curves-480.webp 480w,/assets/img/2023-11-01-innr/initial-model-curves-800.webp 800w,/assets/img/2023-11-01-innr/initial-model-curves-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2023-11-01-innr/initial-model-curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="innr-model-training">INNR Model Training</h3> <p>We must now decide on a coordinate scheme to train our INNR model. This is challenging as the dimensionality of a network changes depending on the type of layer: convolutional layers have 4 dimensions, linear layers have 2 dimensions, and bias layers have a dimension of 1. The simplest mechanism to map these coordinates is to project the lower-dimensional linear and bias layers as a hyperplane centered around the origin. Note that the domain varies significantly across layers, with \((channel\ out,\ channel\ in,\ filter\ height,\ filter\ width)\) as the domain of convolutional layers, \((output,\ input)\) as the domain of linear layers, and \((output)\) as the domain of bias layers.</p> <p>Therefore, our coordinate scheme is as follows:</p> <ul> <li>\((layer,\ c_{out},\ c_{in},\ w,\ h)\) for convolutional layers</li> <li>\((layer,\ out,\ in,\ 0,\ 0)\) for linear layers</li> <li>\((layer,\ out,\ 0,\ 0,\ 0)\) for bias layers (omitted for simplicity)</li> </ul> <p>Given that the domain is variable across layers, it’s likely that the manifold learned by the INNR model is more intricate compared to models dealing with consistent domains, such as images or audio. We normalize the domain of each dimension of a layer to \([-1,\ 1]\). The code for this dataset can be found below.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class ConvCoordinateDataset(Dataset):
  """
  Dataset class for handling neural network coordinates and weights.

  Args:
    params (Tuple[torch.Tensor]): Tuple of parameter tensors
    standardize (bool): Flag indicating whether to standardize the weights.
  """
  def __init__(self, params: Tuple[torch.Tensor], standardize: bool = True):
    self.weights = self._flatten_params(params)
    self.coordinates = self._convert_to_coordinates(params)
    self.standardize = standardize
    if standardize:
      self.transformed_weights = self.standardize_weights()

  # Flatten the paramneter tuple
  def _flatten_params(self, params: Tuple[torch.Tensor]):
    flat_params = torch.concat([param.cpu().flatten() for param in params])
    return flat_params.view(-1, 1)

  # Translate convolutional params into normalized coordinates (cout, cin, h, w)
  def _conv_coordinates(self, param: torch.Tensor) -&gt; torch.Tensor:
    conv_ranges = [torch.linspace(-1, 1, steps=size) for size in param.shape]
    mgrids = torch.meshgrid(*conv_ranges)
    normalized_coordinates = torch.stack(mgrids, dim=-1).reshape(-1, 4)
    return normalized_coordinates

  # Translate bias params into normalized coordinates (bias, 0, 0, 0)
  def _bias_coordinates(self, param: torch.Tensor) -&gt; torch.Tensor:
    zeroes = torch.zeros(1)
    bias_ranges = torch.linspace(-1, 1, steps=param.shape[0])
    mgrids = torch.meshgrid(bias_ranges, zeroes, zeroes, zeroes)
    normalized_coordinates = torch.stack(mgrids, dim=-1).reshape(-1, 4)
    return normalized_coordinates

  # Translate linear parameters into normalized coordinates (out, in, 0, 0)
  def _lin_coordinates(self, param: torch.Tensor) -&gt; torch.Tensor:
    zeroes = torch.zeros(1)
    lin_ranges = [torch.linspace(-1, 1, steps=size) for size in param.shape]
    mgrids = torch.meshgrid(*lin_ranges, zeroes, zeroes)
    normalized_coordinates = torch.stack(mgrids, dim=-1).reshape(-1, 4)
    return normalized_coordinates

  # Generate Coordinates with range [-1, 1] in form (layer, out, in, h, w)
  def _convert_to_coordinates(self, params: Tuple[torch.Tensor]):
    layer_coords = torch.linspace(-1, 1, steps=len(params))
    coordinates = []
    for param in params:
      if param.ndim == 4:
        coordinates.append(self._conv_coordinates(param))
      if param.ndim == 1:
        coordinates.append(self._bias_coordinates(param))
      if param.ndim == 2:
        coordinates.append(self._lin_coordinates(param))
    for i in range(len(params)):
      layer_coord = torch.full((len(coordinates[i]), 1), layer_coords[i])
      coordinates[i] = torch.concat([coordinates[i], layer_coord], -1)
    return torch.concat(coordinates)

  # Standardize outputs to improve training convergence
  def standardize_weights(self):
    mean, std = self.weights.mean(), self.weights.std()
    standardized_weights = (self.weights - mean) / std
    return standardized_weights

  # Reverse standardization for inference time
  def inverse_standardize_weights(self, weights: torch.Tensor):
    mean, std = self.weights.mean(), self.weights.std()
    standardized_weights = weights * std + mean
    return standardized_weights

  def __len__(self):
    return len(self.coordinates)

  def __getitem__(self, idx):
    weights = self.transformed_weights if self.standardize else self.weights
    return self.coordinates[idx], weights[idx]
</code></pre></div></div> <p>The learned INNR model is of the form \(f(l,\ out,\ in,\ x,\ y\ |\ \theta) → \phi_{(l,\ out,\ in,\ x,\ y)}\) where \(\phi\) is the parameter at the given layer coordinates. I use the popular SIREN<d-cite key="sitzmann2020implicit"></d-cite> architecture for the INNR model due to its strong performance on a variety of representation tasks. Unlike the initial model where convergence can be considered arbitrary, for the INNR model, I train with a full <em>2000</em> epochs. However, notably, this training does not require any data used during the training of the initial model, only the raw weights. This means this method could potentially be applied in a variety of settings where data is a protected citizen. Shown below are the loss curves of the INNR Model Training.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-11-01-innr/innr-model-curves-480.webp 480w,/assets/img/2023-11-01-innr/innr-model-curves-800.webp 800w,/assets/img/2023-11-01-innr/innr-model-curves-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2023-11-01-innr/innr-model-curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="results">Results</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-11-01-innr/parameter-comparison-480.webp 480w,/assets/img/2023-11-01-innr/parameter-comparison-800.webp 800w,/assets/img/2023-11-01-innr/parameter-comparison-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2023-11-01-innr/parameter-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As shown above, we see that the performance of the INNR model’s produced model is comparable to the initial model while having roughly 20% fewer parameters. The difference in performance was expected, as INRs are lossy when encoding their mediums. While other techniques may result in smaller networks of comparable quality, what makes this method notable is the exclusion of any <em>training or testing</em> data. The INNR model fully encoded the initial convolutional neural network. However, since we are directly approximating the target network, it does mean that we also reconstruct the potential vulnerabilities of the initial models as well.</p> <h2 id="attacking-the-innr-model">Attacking the INNR model</h2> <p>An adversarial example is a manipulated input data sample that has been perturbed to cause an unexpected outcome. This is typically done by backpropagation through a target network. As our INNR model attempts to approximate the initial network, it too should be vulnerable to any attacks that are generated from the initial network. We test out this hypothesis empirically. For this task, we use the Fast Gradient Sign Method introduced by GoodFellow et al <d-cite key="goodfellow2015explaining"></d-cite>. to generate the adversarial attacks. The Fast Gradient Sign Method is denoted by \(adv_x = x + \epsilon*sign(\nabla_xL(\theta, x, y))\) where \(x\) is the input image, \(y\) is either a directed or undirected adversarial target, \(\theta\) are the parameters of the attacked network, \(adv_x\) is the adversarial instance of \(x\), \(L\) is a loss function, and \(\epsilon\) is the maximum bounded error. Since the attack only considers a single step within an \(\epsilon\) radius ball of the input image, the idea is that the perturbation can be constrained by \(\epsilon\) so as to still appear visually coherent. The code used for the attack is shown below (we only consider undirected attacks).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Generates an adversarial attack using the fast gradient sign method
def attack(model: nn.Module,
          device: torch.device,
          test_loader: DataLoader,
          criterion: nn.Module,
          epsilon=0.1):
  model = model.to(device)
  model.eval()
  attacks, targets = [], []
  for batch_idx, (data, target) in enumerate(test_loader):
    data, target = data.to(device), target.to(device)
    data.requires_grad = True
    output = model(data)
    loss = criterion(output, target)
    model.zero_grad()
    loss.backward()
    attack = data + epsilon * torch.sign(data.grad.data)
    attacks.append(attack)
    targets.append(target)
  return torch.concat(attacks), torch.concat(targets)
</code></pre></div></div> <p>We investigate adversarial attacks to understand the reconstruction error of the INNR model. If the model is faithfully reconstructed then we expect the adversarial attacks to transfer, which we can see in the below figure.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-11-01-innr/attack-comparison-480.webp 480w,/assets/img/2023-11-01-innr/attack-comparison-800.webp 800w,/assets/img/2023-11-01-innr/attack-comparison-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2023-11-01-innr/attack-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>This project was chosen as the code snippet for review for Christopher Wang’s MILA application for Fall of 2024. This project is ideal because of its limited scope, ease of readability, and interesting results. All experiments were conducted on Google Colab using a T4. A link to the the original Google Colab is provided <a href="https://colab.research.google.com/drive/1EI7whJGWOPD1IB6qDr0yK4szOMwaIZ8B?usp=sharing">here</a>.</p>]]></content><author><name>Christopher Wang</name></author><category term="metalearning"/><category term="adversarial"/><summary type="html"><![CDATA[A introduction to Implicit Neural Neural Representation (INNR) models]]></summary></entry></feed>